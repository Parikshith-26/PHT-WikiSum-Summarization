{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrTddY4LoUxE",
    "outputId": "b141629b-a346-40cc-f4e5-8ad5e8fe9a81"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "lyOmR-yRp__x",
    "outputId": "c6dace7b-50ae-45d6-d6e2-d08663fcf9ba"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded=files.upload()\n",
    "print(uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy_nce79tnS0",
    "outputId": "8383342a-79ee-4ce6-e009-0fcf3370a614"
   },
   "outputs": [],
   "source": [
    "!mv /content/WikiSumDataset.jsonl /content/drive/MyDrive/WikiSumDataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yeP6JlQuHPO",
    "outputId": "99df3bb0-4751-4db0-8d3a-a13c39c89bba"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers[torch]\" \"datasets\" \"evaluate\" \"nltk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWv29WBFw9ER",
    "outputId": "8542f77e-2773-4e06-bfb4-cfb8d8982223"
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59abba57",
    "outputId": "9476a814-a04b-47a7-d76f-755df947328d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted /content/WikiSumDataset.jsonl.zip to /content/\n",
      "'/content/WikiSumDataset.jsonl' found after extraction.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file_path = '/content/WikiSumDataset.jsonl.zip'\n",
    "extract_path = '/content/'\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Extracted {zip_file_path} to {extract_path}\")\n",
    "\n",
    "# Verify the file exists after extraction\n",
    "extracted_file_path = os.path.join(extract_path, 'WikiSumDataset.jsonl')\n",
    "if os.path.exists(extracted_file_path):\n",
    "    print(f\"'{extracted_file_path}' found after extraction.\")\n",
    "else:\n",
    "    print(f\"Error: '{extracted_file_path}' not found after extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MYrAxJ6eul1H",
    "outputId": "259d87d0-c8ad-42a5-9738-cc6725d04443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n",
      "Loaded 2000 samples from the WikiSum dataset.\n",
      "Parallel Hierarchical Transformer (PHT) model created successfully.\n",
      "‚úì Hierarchical encoder with segment_size=128\n",
      "‚úì Pretrained weights loaded and properly tied\n",
      "Created 1800 training samples and 200 validation samples.\n",
      "Model moved to cuda.\n",
      "Starting training with the PHT model...\n",
      "üîÑ Training PHT on WikiSum dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 04:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>6.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>6.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>6.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>6.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>6.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>6.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>6.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.069600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PHT training finished!\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   567867GF\n",
      "  train_loss               =     6.3419\n",
      "  train_runtime            = 0:04:10.24\n",
      "  train_samples_per_second =      7.193\n",
      "  train_steps_per_second   =      3.597\n",
      "\n",
      "******** PHT Training Results ********\n",
      "Final training loss: 6.3419\n",
      "Training runtime: 250.2 seconds\n",
      "Samples per second: 7.193\n",
      "\n",
      "üîç Starting PHT model evaluation...\n",
      "Generating summaries for 100 articles with PHT model...\n",
      "Progress: 10/100 | Avg: 1.39s/sample | ETA: 2.1 min\n",
      "Progress: 20/100 | Avg: 1.57s/sample | ETA: 2.1 min\n",
      "Progress: 30/100 | Avg: 1.51s/sample | ETA: 1.8 min\n",
      "Progress: 40/100 | Avg: 1.46s/sample | ETA: 1.5 min\n",
      "Progress: 50/100 | Avg: 1.41s/sample | ETA: 1.2 min\n",
      "Progress: 60/100 | Avg: 1.37s/sample | ETA: 0.9 min\n",
      "Progress: 70/100 | Avg: 1.37s/sample | ETA: 0.7 min\n",
      "Progress: 80/100 | Avg: 1.34s/sample | ETA: 0.4 min\n",
      "Progress: 90/100 | Avg: 1.31s/sample | ETA: 0.2 min\n",
      "Progress: 100/100 | Avg: 1.31s/sample | ETA: 0.0 min\n",
      "\n",
      "============================================================\n",
      "üéØ PHT MODEL ROUGE EVALUATION RESULTS\n",
      "============================================================\n",
      "ROUGE1      : 0.2002\n",
      "ROUGE2      : 0.0143\n",
      "ROUGEL      : 0.1422\n",
      "ROUGELSUM   : 0.1421\n",
      "\n",
      "Evaluation completed in 2.20 minutes\n",
      "Average time per sample: 1.32 seconds\n",
      "\n",
      "============================================================\n",
      "üìÑ SAMPLE PHT GENERATIONS\n",
      "============================================================\n",
      "\n",
      "--- PHT SAMPLE 1 ---\n",
      "üìñ SOURCE ARTICLE (truncated):\n",
      "Slice an aloe vera leaf. You can buy aloe vera plants online or at a local greenhouse. To extract the gel, slice the plant straight down the middle of the leaf. Move the knife slowly to avoid slipping up and accidentally cutting yourself. Make sure to use a sharp knife, as it can be difficult to make a clean cut with a dull blade. Remove the gel. U[...]\n",
      "\n",
      "üéØ REFERENCE SUMMARY:\n",
      "To condition your hair with aloe vera, work some aloe vera gel into your hair in the shower and let it sit for a few minutes. Then, rinse your hair until you've gotten all of the aloe gel out. You can also use aloe vera as a leave-in conditioner. Just mix some aloe vera gel and water in a spray bottle and then spritz your hair with the mixture after you take a shower in the morning.\n",
      "\n",
      "ü§ñ PHT MODEL SUMMARY:\n",
      "Tooeoeoe aloeoe gel al al al gel gel gel a gel a a gel gel, gel a, gel gel the gel gel. gel, a gel,, gel., gel, the gel,. gel. a gel.. gel a. gel gel your gel gel to gel gel al gel. the, gel the. gel the,,,.,..,, a,, the,. the gel a the gel the a gel the the., a., the... a,. a.. the. the the gel. your gel, your gel the your gel.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- PHT SAMPLE 2 ---\n",
      "üìñ SOURCE ARTICLE (truncated):\n",
      "Roast the pistachios in the oven to save time. If you are roasting over ‚ÅÑ 2 cup (120¬†ml) pistachios, use the oven. Choosing the oven will allow you to bake a big batch of pistachios at once. Place the oven rack in the middle of the oven. Have two racks in the oven on top of each other if you are planning to make two pans of pistachios so there is e[...]\n",
      "\n",
      "üéØ REFERENCE SUMMARY:\n",
      "To roast pistachios, start by spreading them out in an even layer on a baking sheet. Then, pop them in the oven, and roast them at 350 degrees Fahrenheit for 7 minutes, or until they become fragrant. You can also roast pistachios on the stovetop. Just place them in a pan over medium heat, and stir them constantly for 7 minutes, or until they turn light brown.\n",
      "\n",
      "ü§ñ PHT MODEL SUMMARY:\n",
      "To oven oven oven, oven oven the oven oven. oven oven a oven oven to oven oven of oven oven and oven oven in oven oven them oven the the oven the, the the the, oven the. the the., the,, the oven, the.. the,. the oven. the. oven,,,.,, oven,..,. oven., oven... oven the a the the and the the a, the a. the and, the and., a the. a the, and the. and the, a,. a,, a., and. the\n",
      "--------------------------------------------------\n",
      "\n",
      "--- PHT SAMPLE 3 ---\n",
      "üìñ SOURCE ARTICLE (truncated):\n",
      "Match the tie length according to your height. Having the wrong size necktie will draw it undue attention, creating imbalance between your tie and outfit. The ideal length for a necktie should hang at about the middle of your belt. Taller individuals will likely need a longer tie, while shorter individuals may need a smaller one. To test your tie l[...]\n",
      "\n",
      "üéØ REFERENCE SUMMARY:\n",
      "To match a tie with your shirt, try pairing a blue shirt with a navy, red, or yellow tie, or a pink shirt with a burgundy or navy tie. Alternatively, choose a tie in a color that complements your hair and skin tone. For example, if you have light hair and light skin, pick a pastel tie, or choose a red tie against a white shirt if you have dark hair and dark skin. You can also look sharp by putting patterns together, such as a large-checked tie with a small-checked shirt in a similar color, or a thickly-striped tie with a thinly-striped shirt.\n",
      "\n",
      "ü§ñ PHT MODEL SUMMARY:\n",
      "To tie tie tie a tie tie the tie tie, tie tie your tie tie. tie tie length tie tie to tie tie ties tie tie and tie tie neck tie tie you tie tie of tie tie Tie tie tie- tie tie pair tie tie with tie tie for tie tie in tie tie or tie tie cut tie tie top tie tie by tie tie make tie tie's tie tie long tie tie at tie tie it tie tie\n",
      "--------------------------------------------------\n",
      "\n",
      "--- PHT SAMPLE 4 ---\n",
      "üìñ SOURCE ARTICLE (truncated):\n",
      "Source garlic to plant. You can always try planting garlic you bought from the grocery store, but you'll have a much higher chance of having a successful crop if you buy garlic cloves, or seeds, from a plant nursery that stocks varieties that grow well in your area. Shop online for a wider selection of garlic and choose one to your liking. Some str[...]\n",
      "\n",
      "üéØ REFERENCE SUMMARY:\n",
      "To plant garlic, choose a spot with full sun and well-draining soil, and use a garden rake or hoe to work the soil to about 4 inches. Divide your garlic clove into individual cloves, keeping the papery skin intact. Plant the cloves with the flat root side pointing down and the tapered side pointing up, and space the cloves about 4 inches apart and about 2 inches deep. Water the garlic every 3-5 days, and cut off any shoots that grow to encourage the garlic bulb to develop. Harvest the garlic when the tops turn yellow and begin to die off.\n",
      "\n",
      "ü§ñ PHT MODEL SUMMARY:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- PHT SAMPLE 5 ---\n",
      "üìñ SOURCE ARTICLE (truncated):\n",
      "Wear a moisturizing lip balm, as needed. Use a balm with shea or cocoa butter for the best results. Reapply it only when your lips feel dry. If you use lip balm too often, it can make your lips more chapped. Apply lip balm underneath any other lip products that you use, including lipstick, lip gloss, and lip stain. Choose the product that sounds mo[...]\n",
      "\n",
      "üéØ REFERENCE SUMMARY:\n",
      "If you want to have soft lips, apply a moisturizing lip balm with shea or cocoa butter whenever your lips begin to feel dry. Try to avoid flavored or dyed lip balms, as they can be drying. If you like some color on your lips, you can try a moisturizing lipstick in your favorite color. Once every 1-2 weeks, exfoliate your lips with a sugar, olive oil, and honey scrub, and condition them with a natural moisturizer like neem oil or milk cream.\n",
      "\n",
      "ü§ñ PHT MODEL SUMMARY:\n",
      "To lip lip lip your lip lip, lip your,,, your, your your your, a,,.,, a your,.., your a, your.,. your, to,, you,, to., a., and,. a,. to, your to,. and,, and., to your, you., you your, and your, the,, lip,, the., lip., the your. your your.. your. a... a your. to.. to your. and.. and your. you,. you.\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "‚úÖ PHT IMPLEMENTATION SUCCESSFULLY COMPLETED!\n",
      "============================================================\n",
      "üî¨ Key PHT Features Implemented:\n",
      "  ‚úì Hierarchical word and paragraph-level encoding\n",
      "  ‚úì Segment-based parallel processing (segment_size=128)\n",
      "  ‚úì Combined representations via residual connections\n",
      "  ‚úì Standard BART decoder for high-quality generation\n",
      "  ‚úì Trained and evaluated on WikiSum dataset\n",
      "\n",
      "üìä Results ready for comparison with PHT research paper!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    BartModel,\n",
    "    BartConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    BartPretrainedModel\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import evaluate\n",
    "import time\n",
    "import json # Import the json library\n",
    "\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA LOADING & PREPARATION\n",
    "# ==============================================================================\n",
    "def load_jsonl(file_path, nrows=None):\n",
    "    \"\"\"Loads a JSONL file into a Pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if nrows is not None and idx >= nrows:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "NUM_SAMPLES = 2000\n",
    "file_path = '/content/WikiSumDataset.jsonl'  # Corrected path to the extracted file\n",
    "df = load_jsonl(file_path, nrows=NUM_SAMPLES)\n",
    "print(f\"Loaded {len(df)} samples from the WikiSum dataset.\")\n",
    "\n",
    "# 3. PARALLEL HIERARCHICAL TRANSFORMER (PHT) MODEL DEFINITION\n",
    "# ==============================================================================\n",
    "class HierarchicalEncoder(nn.Module):\n",
    "    def __init__(self, config: BartConfig, segment_size=128):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.segment_size = segment_size\n",
    "\n",
    "        self.word_level_encoder = BartModel(config).encoder\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.paragraph_level_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "    @property\n",
    "    def embed_tokens(self):\n",
    "        return self.word_level_encoder.embed_tokens\n",
    "\n",
    "    @embed_tokens.setter\n",
    "    def embed_tokens(self, value):\n",
    "        self.word_level_encoder.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        word_level_outputs = self.word_level_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        word_embeddings = word_level_outputs.last_hidden_state\n",
    "        batch_size, seq_len, hidden_size = word_embeddings.shape\n",
    "\n",
    "        effective_seq_len = (seq_len // self.segment_size) * self.segment_size\n",
    "\n",
    "        if effective_seq_len == 0:\n",
    "            return BaseModelOutput(\n",
    "                last_hidden_state=word_embeddings,\n",
    "                hidden_states=word_level_outputs.hidden_states,\n",
    "                attentions=word_level_outputs.attentions,\n",
    "            )\n",
    "\n",
    "        truncated_embeddings = word_embeddings[:, :effective_seq_len, :]\n",
    "        num_segments = effective_seq_len // self.segment_size\n",
    "\n",
    "        segmented_embeddings = truncated_embeddings.reshape(\n",
    "            batch_size, num_segments, self.segment_size, hidden_size\n",
    "        )\n",
    "        paragraph_representations = segmented_embeddings.mean(dim=2)\n",
    "\n",
    "        paragraph_level_outputs = self.paragraph_level_encoder(paragraph_representations)\n",
    "\n",
    "        upsampled_paragraph_outputs = paragraph_level_outputs.repeat_interleave(\n",
    "            self.segment_size, dim=1\n",
    "        )\n",
    "\n",
    "        combined_embeddings = word_embeddings.clone()\n",
    "        combined_embeddings[:, :effective_seq_len, :] += upsampled_paragraph_outputs\n",
    "\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=combined_embeddings,\n",
    "            hidden_states=word_level_outputs.hidden_states,\n",
    "            attentions=word_level_outputs.attentions,\n",
    "        )\n",
    "\n",
    "class PHTModel(BartForConditionalGeneration):\n",
    "    def __init__(self, config, segment_size=128):\n",
    "        super().__init__(config)\n",
    "        self.model.encoder = HierarchicalEncoder(config, segment_size=segment_size)\n",
    "        self.tie_weights()\n",
    "\n",
    "def create_pht_model(model_name=\"facebook/bart-base\", segment_size=128):\n",
    "    base_model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    config = base_model.config\n",
    "    pht_model = PHTModel(config, segment_size=segment_size)\n",
    "\n",
    "    pht_model.model.encoder.word_level_encoder.load_state_dict(base_model.model.encoder.state_dict())\n",
    "    pht_model.model.decoder.load_state_dict(base_model.model.decoder.state_dict())\n",
    "    pht_model.lm_head.load_state_dict(base_model.lm_head.state_dict())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print(\"Parallel Hierarchical Transformer (PHT) model created successfully.\")\n",
    "    print(f\"‚úì Hierarchical encoder with segment_size={segment_size}\")\n",
    "    print(\"‚úì Pretrained weights loaded and properly tied\")\n",
    "\n",
    "    return pht_model, tokenizer\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. DATASET AND DATALOADER\n",
    "# ==============================================================================\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "\n",
    "model, tokenizer = create_pht_model(model_name=MODEL_NAME, segment_size=128)\n",
    "\n",
    "class WikiSumDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.inputs = df['article'].tolist()\n",
    "        self.targets = df['summary'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = str(self.inputs[idx])\n",
    "        summary = str(self.targets[idx])\n",
    "        model_inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels_ids = labels[\"input_ids\"].clone()\n",
    "        labels_ids[labels_ids == self.tokenizer.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels_ids\n",
    "        return {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
    "\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "train_dataset = WikiSumDataset(train_df, tokenizer)\n",
    "val_dataset = WikiSumDataset(val_df, tokenizer)\n",
    "\n",
    "print(f\"Created {len(train_dataset)} training samples and {len(val_dataset)} validation samples.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAINING SETUP AND EXECUTION\n",
    "# ==============================================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Model moved to {device}.\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results_pht',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_pht',\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=4,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training with the PHT model...\")\n",
    "print(\"üîÑ Training PHT on WikiSum dataset...\")\n",
    "train_result = trainer.train()\n",
    "print(\"‚úÖ PHT training finished!\")\n",
    "\n",
    "trainer.save_model()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"\\n******** PHT Training Results ********\")\n",
    "print(f\"Final training loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "print(f\"Training runtime: {metrics.get('train_runtime', 'N/A'):.1f} seconds\")\n",
    "print(f\"Samples per second: {metrics.get('train_samples_per_second', 'N/A'):.3f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PHT INFERENCE AND ROUGE EVALUATION\n",
    "# ==============================================================================\n",
    "print(\"\\nüîç Starting PHT model evaluation...\")\n",
    "rouge = evaluate.load('rouge')\n",
    "N_EVAL_SAMPLES = min(100, len(val_df))\n",
    "\n",
    "eval_articles = val_df['article'].tolist()[:N_EVAL_SAMPLES]\n",
    "gold_summaries = val_df['summary'].tolist()[:N_EVAL_SAMPLES]\n",
    "pred_summaries = []\n",
    "\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Generating summaries for {N_EVAL_SAMPLES} articles with PHT model...\")\n",
    "\n",
    "for i, article in enumerate(eval_articles):\n",
    "    inputs = tokenizer(\n",
    "        article,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=4,\n",
    "            max_length=MAX_TARGET_LENGTH + 2,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    pred_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    pred_summaries.append(pred_summary)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time = elapsed / (i + 1)\n",
    "    if (i + 1) % 10 == 0 or i == N_EVAL_SAMPLES - 1:\n",
    "        eta = avg_time * (N_EVAL_SAMPLES - (i + 1))\n",
    "        print(f\"Progress: {i+1}/{N_EVAL_SAMPLES} | Avg: {avg_time:.2f}s/sample | ETA: {eta/60:.1f} min\")\n",
    "\n",
    "results = rouge.compute(\n",
    "    predictions=pred_summaries,\n",
    "    references=gold_summaries,\n",
    "    use_stemmer=True\n",
    ")\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time) / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PHT MODEL ROUGE EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key.upper():<12}: {value:.4f}\")\n",
    "print(f\"\\nEvaluation completed in {total_time:.2f} minutes\")\n",
    "print(f\"Average time per sample: {(total_time*60)/N_EVAL_SAMPLES:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÑ SAMPLE PHT GENERATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(5, N_EVAL_SAMPLES)):\n",
    "    print(f\"\\n--- PHT SAMPLE {i+1} ---\")\n",
    "    print(f\"üìñ SOURCE ARTICLE (truncated):\")\n",
    "    print(f\"{eval_articles[i][:350]}[...]\")\n",
    "    print(f\"\\nüéØ REFERENCE SUMMARY:\")\n",
    "    print(f\"{gold_summaries[i]}\")\n",
    "    print(f\"\\nü§ñ PHT MODEL SUMMARY:\")\n",
    "    print(f\"{pred_summaries[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PHT IMPLEMENTATION SUCCESSFULLY COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üî¨ Key PHT Features Implemented:\")\n",
    "print(\"  ‚úì Hierarchical word and paragraph-level encoding\")\n",
    "print(\"  ‚úì Segment-based parallel processing (segment_size=128)\")\n",
    "print(\"  ‚úì Combined representations via residual connections\")\n",
    "print(\"  ‚úì Standard BART decoder for high-quality generation\")\n",
    "print(\"  ‚úì Trained and evaluated on WikiSum dataset\")\n",
    "print(\"\\nüìä Results ready for comparison with PHT research paper!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8f40763",
    "outputId": "8669f6e4-0dba-42e2-d078-ce5dac5e832f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ PHT MODEL ROUGE EVALUATION RESULTS\n",
      "============================================================\n",
      "ROUGE1      : 0.2002\n",
      "ROUGE2      : 0.0143\n",
      "ROUGEL      : 0.1422\n",
      "ROUGELSUM   : 0.1421\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PHT MODEL ROUGE EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key.upper():<12}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
